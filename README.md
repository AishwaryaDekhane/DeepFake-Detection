# üîä Robust Audio Deepfake Detection under Laundering Attacks

Spectrogram analysis helps detect deepfakes by visualizing frequency anomalies in audio and video signals. It can be used with machine learning models to identify synthetic patterns that are not naturally occurring. Ongoing research focuses on improving detection accuracy and efficiency, especially against evolving deepfake techniques.

---

## üìö Table of Contents

1. [Overview](#overview)
2. [Key Features](#key-features)
3. [Architecture](#architecture)
   - [Data Flow](#data-flow)
   - [Pipeline Components](#pipeline-components)
4. [Project Structure](#project-structure)
5. [Quick Start](#quick-start)
6. [Usage Examples](#usage-examples)
7. [Configuration](#configuration)
8. [Evaluation & Metrics](#evaluation--metrics)
9. [Development Setup](#development-setup)
10. [Roadmap](#roadmap)
11. [Resources](#resources)
12. [License](#license)

---

## üìå Overview

This project benchmarks deepfake detection systems using audio that has been deliberately distorted through various post-processing attacks. It evaluates models on **spectrogram-transformed audio data** and emphasizes resilience under real-world distortions.

---

## üöÄ Key Features

- üéß Supports audio deepfake detection only (not video)
- üìÅ Trained on ASVSpoof 2019 and curated public figures dataset
- üß™ Evaluated under 5 major laundering categories
- üîç Generates visual plots and performance metrics
- üß† Model: CNN-based architecture

---
## ISSF Lab : https://issf.umd.umich.edu/home 
![Photos](Photos/File1.png)
![Photos](Photos/File4.png)

‚∏ª

‚ö° Quick Start

git clone [https://github.com/AishwaryaDekhane/deepfake-audio-detector.git](https://github.com/AishwaryaDekhane/DeepFake-Detection/tree/main)
cd deepfake-audio-detector
pip install -r requirements.txt
python scripts/train.py


‚∏ª

üß™ Usage Examples

# Convert audio to spectrograms
python scripts/generate_spectrograms.py --input data/raw_audio --output data/spectrograms

# Apply laundering transformations
python scripts/preprocess_laundering.py --input data/raw_audio --output data/laundered

# Train model
python scripts/train.py

# Evaluate model
python scripts/evaluate_models.py

# Visualize results
python scripts/visualize_results.py


‚∏ª

‚öôÔ∏è Configuration

Edit config.yaml to customize model or data parameters:

data_dir: data/spectrograms
model:
  name: Model
  epochs: 50
  lr: 0.0005
  batch_size: 32
save_path: models/saved_model.pth


‚∏ª

üìà Evaluation & Metrics

Replace this with your result tables or plot visuals

	‚Ä¢	Metric 1: Equal Error Rate (EER)
	‚Ä¢	Metric 2: Area Under Curve (AUC)
	‚Ä¢	Tested On:
	‚Ä¢	Reverberation (RT60: 0.3, 0.6, 0.9)
	‚Ä¢	Noise (Babble, White, Street, Cafe, Volvo)
	‚Ä¢	Resampling (8k, 11k, 22k, 44k)
	‚Ä¢	Recompression (16k, 64k ‚Ä¶ 320k)
	‚Ä¢	Low-Pass Filtering (Butterworth 7kHz)

‚∏ª

üßë‚Äçüíª Development Setup
	‚Ä¢	Python: >=3.8
	‚Ä¢	Recommended:
	‚Ä¢	torch
	‚Ä¢	librosa
	‚Ä¢	audiomentations
	‚Ä¢	pyroomacoustics
	‚Ä¢	opencv-python

pip install -r requirements.txt


‚∏ª

üõ£Ô∏è Roadmap
	‚Ä¢	Deepfake classifier
	‚Ä¢	Full laundering attack simulations
	‚Ä¢	Real-time deepfake detection (stream-based)
	‚Ä¢	Web dashboard integration (Streamlit or Flask)
	‚Ä¢	Extend to multi-modal (audio + video)


